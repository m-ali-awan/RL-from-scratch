{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"policy-iteration.ipynb","provenance":[],"authorship_tag":"ABX9TyO8Bez06Qvz3b5QvXhJUZ55"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["> Here we will compute the value function using the policy, but in value iteration method, we compute the value function by taking the maximum over Q values"],"metadata":{"id":"IdrL7fhsgc55"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"-hdGwQpfQOOE","executionInfo":{"status":"ok","timestamp":1641457864240,"user_tz":-300,"elapsed":1558,"user":{"displayName":"Muhammad Ali","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08853600868018567439"}}},"outputs":[],"source":["import gym\n","import numpy as np\n"]},{"cell_type":"code","source":["env = gym.make('FrozenLake-v0')"],"metadata":{"id":"yZhcPUpzgu-e","executionInfo":{"status":"ok","timestamp":1641457871372,"user_tz":-300,"elapsed":1274,"user":{"displayName":"Muhammad Ali","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08853600868018567439"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["> **Algorithm â€“ policy iteration**\n","\n","\n","The steps of the policy iteration algorithm is given as follows:\n","\n","* Initialize a random policy\n","* Compute the value function using the given policy\n","* Extract a new policy using the value function obtained from step 2\n","* If the extracted policy is the same as the policy used in step 2, then stop, else send the extracted new policy to step 2 and repeat steps 2 to 4"],"metadata":{"id":"3t1Py4UgrHzG"}},{"cell_type":"code","source":["def compute_value_function(policy):\n","\n","  num_iterations = 1000\n","  threshold = 1e-20\n","  gamma = 1.0\n","\n","  value_table = np.zeros(env.observation_space.n)\n","\n","  for i in range(num_iterations):\n","\n","    updated_value_table = np.copy(value_table)\n","\n","    for s in range(env.observation_space.n):\n","\n","      # Seleceting the action in the state according to policy\n","      a = policy[s]\n","\n","      value_table[s]= sum([prob * (r + gamma * updated_value_table[s_])\n","                             for prob,s_, r, _ in env.P[s][a]])\n","      \n","\n","    if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n","      break\n","    \n","  return value_table\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"Mdkqi4ZHg16v","executionInfo":{"status":"ok","timestamp":1641459012931,"user_tz":-300,"elapsed":649,"user":{"displayName":"Muhammad Ali","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08853600868018567439"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def extract_policy(value_table):\n","\n","  gamma = 1.0\n","  policy = np.zeros(env.observation_space.n)\n","\n","\n","  for s in range(env.observation_space.n):\n","\n","    Q_values = [\n","                sum([ prob * (r + gamma * value_table[s_])\n","                     for prob, s_, r,_ in env.P[s][a]])\n","                for a in range(env.action_space.n)\n","    ]\n","\n","    # extracting policy by selecting the action which has maximum Q value\n","\n","    policy[s] = np.argmax(np.array(Q_values))\n","\n","  return policy"],"metadata":{"id":"jFWn_OZIlM3-","executionInfo":{"status":"ok","timestamp":1641459431157,"user_tz":-300,"elapsed":632,"user":{"displayName":"Muhammad Ali","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08853600868018567439"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def policy_iteration(env):\n","\n","  num_iterations = 1000\n","  policy = np.zeros(env.observation_space.n)\n","\n","  for i in range(num_iterations):\n","    value_function = compute_value_function(policy)\n","    new_policy = extract_policy(value_function)\n","\n","    if (np.all(policy == new_policy)):\n","      break\n","\n","    policy = new_policy\n","\n","  return policy"],"metadata":{"id":"HvqD2thSpYqO","executionInfo":{"status":"ok","timestamp":1641460270020,"user_tz":-300,"elapsed":629,"user":{"displayName":"Muhammad Ali","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08853600868018567439"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["optimal_policy = policy_iteration(env)"],"metadata":{"id":"P1_Qf7q9my3L","executionInfo":{"status":"ok","timestamp":1641460270651,"user_tz":-300,"elapsed":3,"user":{"displayName":"Muhammad Ali","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08853600868018567439"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["optimal_policy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cx7JnHl2pWpp","executionInfo":{"status":"ok","timestamp":1641460278269,"user_tz":-300,"elapsed":621,"user":{"displayName":"Muhammad Ali","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08853600868018567439"}},"outputId":"263b25db-f130-4c90-d30d-f73089cf5c25"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 3., 3., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":[""],"metadata":{"id":"jFeIvyKPqBzI"},"execution_count":null,"outputs":[]}]}